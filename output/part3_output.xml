<?xml version="1.0"?>
<archive>
  <document name="nlp_data/d08.txt">
    <sentence>
      <token type="NAMED_ENTITY">Apollo 11</token>
      <token type="WORD">was</token>
      <token type="WORD">the</token>
      <token type="WORD">spaceflight</token>
      <token type="WORD">that</token>
      <token type="WORD">landed</token>
      <token type="WORD">the</token>
      <token type="WORD">first</token>
      <token type="WORD">humans</token>
      <token type="WORD">on</token>
      <token type="WORD">the</token>
      <token type="NAMED_ENTITY">Moon</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">Americans</token>
      <token type="NAMED_ENTITY">Neil Armstrong</token>
      <token type="WORD">and</token>
      <token type="NAMED_ENTITY">Buzz Aldrin</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">on</token>
      <token type="WORD">July</token>
      <token type="WORD">20</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">1969</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Armstrong</token>
      <token type="WORD">became</token>
      <token type="WORD">the</token>
      <token type="WORD">first</token>
      <token type="WORD">to</token>
      <token type="WORD">step</token>
      <token type="WORD">onto</token>
      <token type="WORD">the</token>
      <token type="WORD">lunar</token>
      <token type="WORD">surface</token>
      <token type="WORD">six</token>
      <token type="WORD">hours</token>
      <token type="WORD">later</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Armstrong</token>
      <token type="WORD">spent</token>
      <token type="WORD">about</token>
      <token type="WORD">two</token>
      <token type="WORD">and</token>
      <token type="WORD">a</token>
      <token type="WORD">half</token>
      <token type="WORD">hours</token>
      <token type="WORD">outside</token>
      <token type="WORD">the</token>
      <token type="WORD">spacecraft</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">Aldrin</token>
      <token type="WORD">slightly</token>
      <token type="WORD">less</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">and</token>
      <token type="WORD">together</token>
      <token type="WORD">they</token>
      <token type="WORD">collected</token>
      <token type="WORD">47</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">5</token>
      <token type="WORD">pounds</token>
      <token type="PUNCTUATION"><![CDATA[(]]></token>
      <token type="WORD">21</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">5</token>
      <token type="WORD">kg</token>
      <token type="PUNCTUATION"><![CDATA[)]]></token>
      <token type="WORD">of</token>
      <token type="WORD">lunar</token>
      <token type="WORD">material</token>
      <token type="WORD">for</token>
      <token type="WORD">return</token>
      <token type="WORD">to</token>
      <token type="NAMED_ENTITY">Earth</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">The</token>
      <token type="WORD">third</token>
      <token type="WORD">member</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="WORD">mission</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="NAMED_ENTITY">Michael Collins</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">piloted</token>
      <token type="WORD">the</token>
      <token type="WORD">command</token>
      <token type="WORD">spacecraft</token>
      <token type="WORD">alone</token>
      <token type="WORD">in</token>
      <token type="WORD">lunar</token>
      <token type="WORD">orbit</token>
      <token type="WORD">until</token>
      <token type="WORD">Armstrong</token>
      <token type="WORD">and</token>
      <token type="WORD">Aldrin</token>
      <token type="WORD">returned</token>
      <token type="WORD">to</token>
      <token type="WORD">it</token>
      <token type="WORD">just</token>
      <token type="WORD">under</token>
      <token type="WORD">a</token>
      <token type="WORD">day</token>
      <token type="WORD">later</token>
      <token type="WORD">for</token>
      <token type="WORD">the</token>
      <token type="WORD">trip</token>
      <token type="WORD">back</token>
      <token type="WORD">to</token>
      <token type="NAMED_ENTITY">Earth</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
  </document>
  <document name="nlp_data/d05.txt">
    <sentence>
      <token type="WORD">Horseshoe</token>
      <token type="WORD">crabs</token>
      <token type="WORD">are</token>
      <token type="WORD">marine</token>
      <token type="WORD">arthropods</token>
      <token type="WORD">that</token>
      <token type="WORD">live</token>
      <token type="WORD">primarily</token>
      <token type="WORD">in</token>
      <token type="WORD">and</token>
      <token type="WORD">around</token>
      <token type="WORD">shallow</token>
      <token type="WORD">ocean</token>
      <token type="WORD">waters</token>
      <token type="WORD">on</token>
      <token type="WORD">soft</token>
      <token type="WORD">sandy</token>
      <token type="WORD">or</token>
      <token type="WORD">muddy</token>
      <token type="WORD">bottoms</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">They</token>
      <token type="WORD">occasionally</token>
      <token type="WORD">come</token>
      <token type="WORD">onto</token>
      <token type="WORD">shore</token>
      <token type="WORD">to</token>
      <token type="WORD">mate</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">They</token>
      <token type="WORD">are</token>
      <token type="WORD">commonly</token>
      <token type="WORD">used</token>
      <token type="WORD">as</token>
      <token type="WORD">bait</token>
      <token type="WORD">and</token>
      <token type="WORD">in</token>
      <token type="WORD">fertilizer</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">In</token>
      <token type="WORD">recent</token>
      <token type="WORD">years</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">a</token>
      <token type="WORD">decline</token>
      <token type="WORD">in</token>
      <token type="WORD">the</token>
      <token type="WORD">population</token>
      <token type="WORD">has</token>
      <token type="WORD">occurred</token>
      <token type="WORD">as</token>
      <token type="WORD">a</token>
      <token type="WORD">consequence</token>
      <token type="WORD">of</token>
      <token type="WORD">coastal</token>
      <token type="WORD">habitat</token>
      <token type="WORD">destruction</token>
      <token type="WORD">in</token>
      <token type="NAMED_ENTITY">Japan</token>
      <token type="WORD">and</token>
      <token type="WORD">overharvesting</token>
      <token type="WORD">along</token>
      <token type="WORD">the</token>
      <token type="WORD">east</token>
      <token type="WORD">coast</token>
      <token type="WORD">of</token>
      <token type="NAMED_ENTITY">North America</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Because</token>
      <token type="WORD">of</token>
      <token type="WORD">their</token>
      <token type="WORD">origin</token>
      <token type="WORD">450</token>
      <token type="WORD">million</token>
      <token type="WORD">years</token>
      <token type="WORD">ago</token>
      <token type="PUNCTUATION"><![CDATA[(]]></token>
      <token type="WORD">Mya</token>
      <token type="PUNCTUATION"><![CDATA[)]]></token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">horseshoe</token>
      <token type="WORD">crabs</token>
      <token type="WORD">are</token>
      <token type="WORD">considered</token>
      <token type="WORD">living</token>
      <token type="WORD">fossils</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Horseshoe</token>
      <token type="WORD">crabs</token>
      <token type="WORD">resemble</token>
      <token type="WORD">crustaceans</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">but</token>
      <token type="WORD">belong</token>
      <token type="WORD">to</token>
      <token type="WORD">a</token>
      <token type="WORD">separate</token>
      <token type="WORD">subphylum</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">and</token>
      <token type="WORD">are</token>
      <token type="WORD">closely</token>
      <token type="WORD">related</token>
      <token type="WORD">to</token>
      <token type="WORD">arachnids</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">The</token>
      <token type="WORD">earliest</token>
      <token type="WORD">horseshoe</token>
      <token type="WORD">crab</token>
      <token type="WORD">fossils</token>
      <token type="WORD">are</token>
      <token type="WORD">found</token>
      <token type="WORD">in</token>
      <token type="WORD">strata</token>
      <token type="WORD">from</token>
      <token type="WORD">the</token>
      <token type="WORD">late</token>
      <token type="WORD">Ordovician</token>
      <token type="WORD">period</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">roughly</token>
      <token type="WORD">450</token>
      <token type="WORD">Mya</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
  </document>
  <document name="nlp_data/d03.txt">
    <sentence>
      <token type="NAMED_ENTITY">Montgomery Castle</token>
      <token type="WORD">is</token>
      <token type="WORD">a</token>
      <token type="WORD">stone</token>
      <token type="WORD">masonry</token>
      <token type="WORD">castle</token>
      <token type="WORD">looking</token>
      <token type="WORD">over</token>
      <token type="WORD">the</token>
      <token type="WORD">town</token>
      <token type="WORD">of</token>
      <token type="WORD">Montgomery</token>
      <token type="WORD">in</token>
      <token type="NAMED_ENTITY">Powys</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="NAMED_ENTITY">Wales</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">It</token>
      <token type="WORD">is</token>
      <token type="WORD">one</token>
      <token type="WORD">of</token>
      <token type="WORD">many</token>
      <token type="WORD">Norman</token>
      <token type="WORD">castles</token>
      <token type="WORD">on</token>
      <token type="WORD">the</token>
      <token type="WORD">border</token>
      <token type="WORD">between</token>
      <token type="NAMED_ENTITY">Wales</token>
      <token type="WORD">and</token>
      <token type="NAMED_ENTITY">England</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">The</token>
      <token type="WORD">original</token>
      <token type="WORD">castle</token>
      <token type="WORD">was</token>
      <token type="WORD">built</token>
      <token type="WORD">at</token>
      <token type="WORD">the</token>
      <token type="WORD">order</token>
      <token type="WORD">of</token>
      <token type="NAMED_ENTITY">Roger de Montgomery</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">earl</token>
      <token type="WORD">of</token>
      <token type="NAMED_ENTITY">Shrewsbury</token>
      <token type="WORD">sometime</token>
      <token type="WORD">between</token>
      <token type="WORD">1071</token>
      <token type="WORD">and</token>
      <token type="WORD">1074</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">On</token>
      <token type="WORD">the</token>
      <token type="WORD">rebellion</token>
      <token type="WORD">of</token>
      <token type="WORD">his</token>
      <token type="WORD">son</token>
      <token type="NAMED_ENTITY">Robert of Belleme</token>
      <token type="WORD">in</token>
      <token type="WORD">1102</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">the</token>
      <token type="WORD">castle</token>
      <token type="WORD">was</token>
      <token type="WORD">given</token>
      <token type="WORD">to</token>
      <token type="WORD">Baldwin</token>
      <token type="WORD">de</token>
      <token type="WORD">Boulers</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">In</token>
      <token type="WORD">1267</token>
      <token type="WORD">Montgomery</token>
      <token type="WORD">was</token>
      <token type="WORD">the</token>
      <token type="WORD">meeting</token>
      <token type="WORD">place</token>
      <token type="WORD">for</token>
      <token type="WORD">treaty</token>
      <token type="WORD">negotiations</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">where</token>
      <token type="NAMED_ENTITY">King Henry III</token>
      <token type="WORD">granted</token>
      <token type="NAMED_ENTITY">Llywelyn ap Gruffudd</token>
      <token type="WORD">the</token>
      <token type="WORD">title</token>
      <token type="WORD">of</token>
      <token type="NAMED_ENTITY">Prince of Wales</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">After</token>
      <token type="WORD">1295</token>
      <token type="WORD">and</token>
      <token type="WORD">the</token>
      <token type="WORD">final</token>
      <token type="WORD">Welsh</token>
      <token type="WORD">War</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="WORD">thirteenth</token>
      <token type="WORD">century</token>
      <token type="WORD">the</token>
      <token type="WORD">castle</token>
      <token type="WORD">became</token>
      <token type="WORD">more</token>
      <token type="WORD">of</token>
      <token type="WORD">a</token>
      <token type="WORD">military</token>
      <token type="WORD">backwater</token>
      <token type="WORD">and</token>
      <token type="WORD">prison</token>
      <token type="WORD">than</token>
      <token type="WORD">a</token>
      <token type="WORD">front</token>
      <token type="WORD">line</token>
      <token type="WORD">fortress</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
  </document>
  <document name="nlp_data/d06.txt">
    <sentence>
      <token type="WORD">A</token>
      <token type="WORD">differentiable</token>
      <token type="WORD">manifold</token>
      <token type="WORD">is</token>
      <token type="WORD">a</token>
      <token type="WORD">type</token>
      <token type="WORD">of</token>
      <token type="WORD">manifold</token>
      <token type="WORD">that</token>
      <token type="WORD">is</token>
      <token type="WORD">locally</token>
      <token type="WORD">similar</token>
      <token type="WORD">enough</token>
      <token type="WORD">to</token>
      <token type="WORD">a</token>
      <token type="WORD">linear</token>
      <token type="WORD">space</token>
      <token type="WORD">to</token>
      <token type="WORD">allow</token>
      <token type="WORD">one</token>
      <token type="WORD">to</token>
      <token type="WORD">do</token>
      <token type="WORD">calculus</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Any</token>
      <token type="WORD">manifold</token>
      <token type="WORD">can</token>
      <token type="WORD">be</token>
      <token type="WORD">described</token>
      <token type="WORD">by</token>
      <token type="WORD">a</token>
      <token type="WORD">collection</token>
      <token type="WORD">of</token>
      <token type="WORD">charts</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">also</token>
      <token type="WORD">known</token>
      <token type="WORD">as</token>
      <token type="WORD">an</token>
      <token type="WORD">atlas</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">One</token>
      <token type="WORD">may</token>
      <token type="WORD">then</token>
      <token type="WORD">apply</token>
      <token type="WORD">ideas</token>
      <token type="WORD">from</token>
      <token type="WORD">calculus</token>
      <token type="WORD">while</token>
      <token type="WORD">working</token>
      <token type="WORD">within</token>
      <token type="WORD">the</token>
      <token type="WORD">individual</token>
      <token type="WORD">charts</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">since</token>
      <token type="WORD">each</token>
      <token type="WORD">chart</token>
      <token type="WORD">lies</token>
      <token type="WORD">within</token>
      <token type="WORD">a</token>
      <token type="WORD">linear</token>
      <token type="WORD">space</token>
      <token type="WORD">to</token>
      <token type="WORD">which</token>
      <token type="WORD">the</token>
      <token type="WORD">usual</token>
      <token type="WORD">rules</token>
      <token type="WORD">of</token>
      <token type="WORD">calculus</token>
      <token type="WORD">apply</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">If</token>
      <token type="WORD">the</token>
      <token type="WORD">charts</token>
      <token type="WORD">are</token>
      <token type="WORD">suitably</token>
      <token type="WORD">compatible</token>
      <token type="PUNCTUATION"><![CDATA[(]]></token>
      <token type="WORD">namely</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">the</token>
      <token type="WORD">transition</token>
      <token type="WORD">from</token>
      <token type="WORD">one</token>
      <token type="WORD">chart</token>
      <token type="WORD">to</token>
      <token type="WORD">another</token>
      <token type="WORD">is</token>
      <token type="WORD">differentiable</token>
      <token type="PUNCTUATION"><![CDATA[)]]></token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">then</token>
      <token type="WORD">computations</token>
      <token type="WORD">done</token>
      <token type="WORD">in</token>
      <token type="WORD">one</token>
      <token type="WORD">chart</token>
      <token type="WORD">are</token>
      <token type="WORD">valid</token>
      <token type="WORD">in</token>
      <token type="WORD">any</token>
      <token type="WORD">other</token>
      <token type="WORD">differentiable</token>
      <token type="WORD">chart</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
  </document>
  <document name="nlp_data/d01.txt">
    <sentence>
      <token type="NAMED_ENTITY">Euclid</token>
      <token type="PUNCTUATION"><![CDATA[']]></token>
      <token type="WORD">s</token>
      <token type="NAMED_ENTITY">Elements</token>
      <token type="WORD">has</token>
      <token type="WORD">been</token>
      <token type="WORD">referred</token>
      <token type="WORD">to</token>
      <token type="WORD">as</token>
      <token type="WORD">the</token>
      <token type="WORD">most</token>
      <token type="WORD">successful</token>
      <token type="WORD">and</token>
      <token type="WORD">influential</token>
      <token type="WORD">textbook</token>
      <token type="WORD">ever</token>
      <token type="WORD">written</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">First</token>
      <token type="WORD">set</token>
      <token type="WORD">in</token>
      <token type="WORD">type</token>
      <token type="WORD">in</token>
      <token type="NAMED_ENTITY">Venice</token>
      <token type="WORD">in</token>
      <token type="WORD">1482</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">it</token>
      <token type="WORD">is</token>
      <token type="WORD">one</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="WORD">very</token>
      <token type="WORD">earliest</token>
      <token type="WORD">mathematical</token>
      <token type="WORD">works</token>
      <token type="WORD">to</token>
      <token type="WORD">be</token>
      <token type="WORD">printed</token>
      <token type="WORD">after</token>
      <token type="WORD">the</token>
      <token type="WORD">invention</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="WORD">printing</token>
      <token type="WORD">press</token>
      <token type="WORD">and</token>
      <token type="WORD">was</token>
      <token type="WORD">estimated</token>
      <token type="WORD">by</token>
      <token type="NAMED_ENTITY">Carl Benjamin Boyer</token>
      <token type="WORD">to</token>
      <token type="WORD">be</token>
      <token type="WORD">second</token>
      <token type="WORD">only</token>
      <token type="WORD">to</token>
      <token type="WORD">the</token>
      <token type="NAMED_ENTITY">Bible</token>
      <token type="WORD">in</token>
      <token type="WORD">the</token>
      <token type="WORD">number</token>
      <token type="WORD">of</token>
      <token type="WORD">editions</token>
      <token type="WORD">published</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">with</token>
      <token type="WORD">the</token>
      <token type="WORD">number</token>
      <token type="WORD">reaching</token>
      <token type="WORD">well</token>
      <token type="WORD">over</token>
      <token type="WORD">one</token>
      <token type="WORD">thousand</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">For</token>
      <token type="WORD">centuries</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">knowledge</token>
      <token type="WORD">of</token>
      <token type="WORD">at</token>
      <token type="WORD">least</token>
      <token type="WORD">part</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="NAMED_ENTITY">Elements</token>
      <token type="WORD">was</token>
      <token type="WORD">required</token>
      <token type="WORD">of</token>
      <token type="WORD">all</token>
      <token type="WORD">students</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Not</token>
      <token type="WORD">until</token>
      <token type="WORD">the</token>
      <token type="WORD">20th</token>
      <token type="WORD">century</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">by</token>
      <token type="WORD">which</token>
      <token type="WORD">time</token>
      <token type="WORD">its</token>
      <token type="WORD">content</token>
      <token type="WORD">was</token>
      <token type="WORD">universally</token>
      <token type="WORD">taught</token>
      <token type="WORD">through</token>
      <token type="WORD">other</token>
      <token type="WORD">school</token>
      <token type="WORD">textbooks</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">did</token>
      <token type="WORD">it</token>
      <token type="WORD">cease</token>
      <token type="WORD">to</token>
      <token type="WORD">be</token>
      <token type="WORD">considered</token>
      <token type="WORD">something</token>
      <token type="WORD">all</token>
      <token type="WORD">educated</token>
      <token type="WORD">people</token>
      <token type="WORD">had</token>
      <token type="WORD">read</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
  </document>
  <document name="nlp_data/d09.txt">
    <sentence>
      <token type="WORD">The</token>
      <token type="WORD">Apollo</token>
      <token type="WORD">spacecraft</token>
      <token type="WORD">had</token>
      <token type="WORD">three</token>
      <token type="WORD">parts</token>
      <token type="PUNCTUATION"><![CDATA[:]]></token>
      <token type="WORD">a</token>
      <token type="WORD">Command</token>
      <token type="WORD">Module</token>
      <token type="WORD">with</token>
      <token type="WORD">a</token>
      <token type="WORD">cabin</token>
      <token type="WORD">for</token>
      <token type="WORD">the</token>
      <token type="WORD">three</token>
      <token type="WORD">astronauts</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">and</token>
      <token type="WORD">the</token>
      <token type="WORD">only</token>
      <token type="WORD">part</token>
      <token type="WORD">that</token>
      <token type="WORD">landed</token>
      <token type="WORD">back</token>
      <token type="WORD">on</token>
      <token type="NAMED_ENTITY">Earth</token>
      <token type="PUNCTUATION"><![CDATA[;]]></token>
      <token type="WORD">a</token>
      <token type="WORD">Service</token>
      <token type="WORD">Module</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">which</token>
      <token type="WORD">supported</token>
      <token type="WORD">the</token>
      <token type="WORD">Command</token>
      <token type="WORD">Module</token>
      <token type="WORD">with</token>
      <token type="WORD">propulsion</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">electrical</token>
      <token type="WORD">power</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">oxygen</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">and</token>
      <token type="WORD">water</token>
      <token type="PUNCTUATION"><![CDATA[;]]></token>
      <token type="WORD">and</token>
      <token type="WORD">a</token>
      <token type="WORD">Lunar</token>
      <token type="WORD">Module</token>
      <token type="WORD">for</token>
      <token type="WORD">landing</token>
      <token type="WORD">on</token>
      <token type="WORD">the</token>
      <token type="NAMED_ENTITY">Moon</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">After</token>
      <token type="WORD">being</token>
      <token type="WORD">sent</token>
      <token type="WORD">toward</token>
      <token type="WORD">the</token>
      <token type="NAMED_ENTITY">Moon</token>
      <token type="WORD">by</token>
      <token type="WORD">the</token>
      <token type="WORD">Saturn</token>
      <token type="WORD">V</token>
      <token type="PUNCTUATION"><![CDATA[']]></token>
      <token type="WORD">s</token>
      <token type="WORD">upper</token>
      <token type="WORD">stage</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">the</token>
      <token type="WORD">astronauts</token>
      <token type="WORD">separated</token>
      <token type="WORD">the</token>
      <token type="WORD">spacecraft</token>
      <token type="WORD">from</token>
      <token type="WORD">it</token>
      <token type="WORD">and</token>
      <token type="WORD">traveled</token>
      <token type="WORD">for</token>
      <token type="WORD">three</token>
      <token type="WORD">days</token>
      <token type="WORD">until</token>
      <token type="WORD">they</token>
      <token type="WORD">entered</token>
      <token type="WORD">into</token>
      <token type="WORD">lunar</token>
      <token type="WORD">orbit</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Armstrong</token>
      <token type="WORD">and</token>
      <token type="WORD">Aldrin</token>
      <token type="WORD">then</token>
      <token type="WORD">moved</token>
      <token type="WORD">into</token>
      <token type="WORD">the</token>
      <token type="WORD">Lunar</token>
      <token type="WORD">Module</token>
      <token type="WORD">and</token>
      <token type="WORD">landed</token>
      <token type="WORD">in</token>
      <token type="WORD">the</token>
      <token type="NAMED_ENTITY">Sea of Tranquility</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">They</token>
      <token type="WORD">stayed</token>
      <token type="WORD">a</token>
      <token type="WORD">total</token>
      <token type="WORD">of</token>
      <token type="WORD">about</token>
      <token type="WORD">21</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">5</token>
      <token type="WORD">hours</token>
      <token type="WORD">on</token>
      <token type="WORD">the</token>
      <token type="WORD">lunar</token>
      <token type="WORD">surface</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">After</token>
      <token type="WORD">lifting</token>
      <token type="WORD">off</token>
      <token type="WORD">in</token>
      <token type="WORD">the</token>
      <token type="WORD">upper</token>
      <token type="WORD">part</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="WORD">Lunar</token>
      <token type="WORD">Module</token>
      <token type="WORD">and</token>
      <token type="WORD">rejoining</token>
      <token type="WORD">Collins</token>
      <token type="WORD">in</token>
      <token type="WORD">the</token>
      <token type="WORD">Command</token>
      <token type="WORD">Module</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">they</token>
      <token type="WORD">returned</token>
      <token type="WORD">to</token>
      <token type="NAMED_ENTITY">Earth</token>
      <token type="WORD">and</token>
      <token type="WORD">landed</token>
      <token type="WORD">in</token>
      <token type="WORD">the</token>
      <token type="NAMED_ENTITY">Pacific Ocean</token>
      <token type="WORD">on</token>
      <token type="WORD">July</token>
      <token type="WORD">24</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
  </document>
  <document name="nlp_data/d04.txt">
    <sentence>
      <token type="WORD">The</token>
      <token type="NAMED_ENTITY">Antikythera</token>
      <token type="WORD">mechanism</token>
      <token type="WORD">is</token>
      <token type="WORD">an</token>
      <token type="WORD">ancient</token>
      <token type="WORD">analog</token>
      <token type="WORD">computer</token>
      <token type="WORD">designed</token>
      <token type="WORD">to</token>
      <token type="WORD">predict</token>
      <token type="WORD">astronomical</token>
      <token type="WORD">positions</token>
      <token type="WORD">and</token>
      <token type="WORD">eclipses</token>
      <token type="WORD">for</token>
      <token type="WORD">calendrical</token>
      <token type="WORD">and</token>
      <token type="WORD">astrological</token>
      <token type="WORD">purposes</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">as</token>
      <token type="WORD">well</token>
      <token type="WORD">as</token>
      <token type="WORD">the</token>
      <token type="WORD">cycles</token>
      <token type="WORD">of</token>
      <token type="NAMED_ENTITY">Olympic Games</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Found</token>
      <token type="WORD">housed</token>
      <token type="WORD">in</token>
      <token type="WORD">a</token>
      <token type="WORD">340</token>
      <token type="PUNCTUATION"><![CDATA[▒]]></token>
      <token type="PUNCTUATION"><![CDATA[▒]]></token>
      <token type="WORD">180</token>
      <token type="PUNCTUATION"><![CDATA[▒]]></token>
      <token type="PUNCTUATION"><![CDATA[▒]]></token>
      <token type="WORD">90</token>
      <token type="WORD">mm</token>
      <token type="WORD">wooden</token>
      <token type="WORD">box</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">the</token>
      <token type="WORD">device</token>
      <token type="WORD">is</token>
      <token type="WORD">a</token>
      <token type="WORD">complex</token>
      <token type="WORD">clockwork</token>
      <token type="WORD">mechanism</token>
      <token type="WORD">composed</token>
      <token type="WORD">of</token>
      <token type="WORD">at</token>
      <token type="WORD">least</token>
      <token type="WORD">30</token>
      <token type="WORD">meshing</token>
      <token type="WORD">bronze</token>
      <token type="WORD">gears</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Its</token>
      <token type="WORD">remains</token>
      <token type="WORD">were</token>
      <token type="WORD">found</token>
      <token type="WORD">as</token>
      <token type="WORD">82</token>
      <token type="WORD">separate</token>
      <token type="WORD">fragments</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">of</token>
      <token type="WORD">which</token>
      <token type="WORD">only</token>
      <token type="WORD">seven</token>
      <token type="WORD">contain</token>
      <token type="WORD">any</token>
      <token type="WORD">gears</token>
      <token type="WORD">or</token>
      <token type="WORD">significant</token>
      <token type="WORD">inscriptions</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">The</token>
      <token type="WORD">largest</token>
      <token type="WORD">gear</token>
      <token type="WORD">is</token>
      <token type="WORD">approximately</token>
      <token type="WORD">140</token>
      <token type="WORD">mm</token>
      <token type="WORD">in</token>
      <token type="WORD">diameter</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">The</token>
      <token type="WORD">artifact</token>
      <token type="WORD">was</token>
      <token type="WORD">recovered</token>
      <token type="WORD">in</token>
      <token type="WORD">1901</token>
      <token type="WORD">from</token>
      <token type="WORD">the</token>
      <token type="NAMED_ENTITY">Antikythera</token>
      <token type="WORD">shipwreck</token>
      <token type="WORD">off</token>
      <token type="WORD">the</token>
      <token type="WORD">Greek</token>
      <token type="WORD">island</token>
      <token type="WORD">of</token>
      <token type="NAMED_ENTITY">Antikythera</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Believed</token>
      <token type="WORD">to</token>
      <token type="WORD">have</token>
      <token type="WORD">been</token>
      <token type="WORD">designed</token>
      <token type="WORD">and</token>
      <token type="WORD">constructed</token>
      <token type="WORD">by</token>
      <token type="WORD">Greek</token>
      <token type="WORD">scientists</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">the</token>
      <token type="WORD">instrument</token>
      <token type="WORD">has</token>
      <token type="WORD">been</token>
      <token type="WORD">dated</token>
      <token type="WORD">either</token>
      <token type="WORD">between</token>
      <token type="WORD">150</token>
      <token type="WORD">to</token>
      <token type="WORD">100</token>
      <token type="WORD">BC</token>
      <token type="WORD">or</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">according</token>
      <token type="WORD">to</token>
      <token type="WORD">a</token>
      <token type="WORD">more</token>
      <token type="WORD">recent</token>
      <token type="WORD">view</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">at</token>
      <token type="WORD">205</token>
      <token type="WORD">BC</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
  </document>
  <document name="nlp_data/d07.txt">
    <sentence>
      <token type="NAMED_ENTITY">Sun Microsystems</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">Inc</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">was</token>
      <token type="WORD">a</token>
      <token type="WORD">company</token>
      <token type="WORD">that</token>
      <token type="WORD">sold</token>
      <token type="WORD">computers</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">computer</token>
      <token type="WORD">components</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">computer</token>
      <token type="WORD">software</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">and</token>
      <token type="WORD">information</token>
      <token type="WORD">technology</token>
      <token type="WORD">services</token>
      <token type="WORD">and</token>
      <token type="WORD">that</token>
      <token type="WORD">created</token>
      <token type="WORD">the</token>
      <token type="WORD">Java</token>
      <token type="WORD">programming</token>
      <token type="WORD">language</token>
      <token type="WORD">and</token>
      <token type="WORD">the</token>
      <token type="WORD">Network</token>
      <token type="WORD">File</token>
      <token type="WORD">System</token>
      <token type="PUNCTUATION"><![CDATA[(]]></token>
      <token type="WORD">NFS</token>
      <token type="PUNCTUATION"><![CDATA[)]]></token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="NAMED_ENTITY">Sun</token>
      <token type="WORD">significantly</token>
      <token type="WORD">evolved</token>
      <token type="WORD">several</token>
      <token type="WORD">key</token>
      <token type="WORD">computing</token>
      <token type="WORD">technologies</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">among</token>
      <token type="WORD">them</token>
      <token type="WORD">Unix</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">RISC</token>
      <token type="WORD">processors</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">thin</token>
      <token type="WORD">client</token>
      <token type="WORD">computing</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">and</token>
      <token type="WORD">virtualized</token>
      <token type="WORD">computing</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="NAMED_ENTITY">Sun</token>
      <token type="WORD">was</token>
      <token type="WORD">founded</token>
      <token type="WORD">on</token>
      <token type="WORD">February</token>
      <token type="WORD">24</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">1982</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">On</token>
      <token type="WORD">January</token>
      <token type="WORD">27</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">2010</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="NAMED_ENTITY">Sun</token>
      <token type="WORD">was</token>
      <token type="WORD">acquired</token>
      <token type="WORD">by</token>
      <token type="NAMED_ENTITY">Oracle Corporation</token>
      <token type="WORD">for</token>
      <token type="WORD">US</token>
      <token type="PUNCTUATION"><![CDATA[$]]></token>
      <token type="WORD">7</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">4</token>
      <token type="WORD">billion</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">The</token>
      <token type="WORD">following</token>
      <token type="WORD">month</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="NAMED_ENTITY">Sun</token>
      <token type="WORD">was</token>
      <token type="WORD">merged</token>
      <token type="WORD">with</token>
      <token type="WORD">Oracle</token>
      <token type="WORD">USA</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">Inc</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">to</token>
      <token type="WORD">become</token>
      <token type="WORD">Oracle</token>
      <token type="WORD">America</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">Inc</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
  </document>
  <document name="nlp_data/d02.txt">
    <sentence>
      <token type="WORD">The</token>
      <token type="NAMED_ENTITY">Broyden–Fletcher–Goldfarb–Shanno</token>
      <token type="PUNCTUATION"><![CDATA[(]]></token>
      <token type="NAMED_ENTITY">BFGS</token>
      <token type="PUNCTUATION"><![CDATA[)]]></token>
      <token type="WORD">algorithm</token>
      <token type="WORD">is</token>
      <token type="WORD">an</token>
      <token type="WORD">iterative</token>
      <token type="WORD">method</token>
      <token type="WORD">for</token>
      <token type="WORD">solving</token>
      <token type="WORD">unconstrained</token>
      <token type="WORD">nonlinear</token>
      <token type="WORD">optimization</token>
      <token type="WORD">problems</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">The</token>
      <token type="NAMED_ENTITY">BFGS</token>
      <token type="WORD">method</token>
      <token type="WORD">approximates</token>
      <token type="NAMED_ENTITY">Newton</token>
      <token type="PUNCTUATION"><![CDATA[']]></token>
      <token type="WORD">s</token>
      <token type="WORD">method</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">a</token>
      <token type="WORD">class</token>
      <token type="WORD">of</token>
      <token type="WORD">hill-climbing</token>
      <token type="WORD">optimization</token>
      <token type="WORD">techniques</token>
      <token type="WORD">that</token>
      <token type="WORD">seeks</token>
      <token type="WORD">a</token>
      <token type="WORD">stationary</token>
      <token type="WORD">point</token>
      <token type="WORD">of</token>
      <token type="WORD">a</token>
      <token type="PUNCTUATION"><![CDATA[(]]></token>
      <token type="WORD">preferably</token>
      <token type="WORD">twice</token>
      <token type="WORD">continuously</token>
      <token type="WORD">differentiable</token>
      <token type="PUNCTUATION"><![CDATA[)]]></token>
      <token type="WORD">function</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">For</token>
      <token type="WORD">such</token>
      <token type="WORD">problems</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">a</token>
      <token type="WORD">necessary</token>
      <token type="WORD">condition</token>
      <token type="WORD">for</token>
      <token type="WORD">optimality</token>
      <token type="WORD">is</token>
      <token type="WORD">that</token>
      <token type="WORD">the</token>
      <token type="WORD">gradient</token>
      <token type="WORD">be</token>
      <token type="WORD">zero</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="NAMED_ENTITY">Newton</token>
      <token type="PUNCTUATION"><![CDATA[']]></token>
      <token type="WORD">s</token>
      <token type="WORD">method</token>
      <token type="WORD">and</token>
      <token type="WORD">the</token>
      <token type="NAMED_ENTITY">BFGS</token>
      <token type="WORD">methods</token>
      <token type="WORD">are</token>
      <token type="WORD">not</token>
      <token type="WORD">guaranteed</token>
      <token type="WORD">to</token>
      <token type="WORD">converge</token>
      <token type="WORD">unless</token>
      <token type="WORD">the</token>
      <token type="WORD">function</token>
      <token type="WORD">has</token>
      <token type="WORD">a</token>
      <token type="WORD">quadratic</token>
      <token type="WORD">Taylor</token>
      <token type="WORD">expansion</token>
      <token type="WORD">near</token>
      <token type="WORD">an</token>
      <token type="WORD">optimum</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">These</token>
      <token type="WORD">methods</token>
      <token type="WORD">use</token>
      <token type="WORD">both</token>
      <token type="WORD">the</token>
      <token type="WORD">first</token>
      <token type="WORD">and</token>
      <token type="WORD">second</token>
      <token type="WORD">derivatives</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="WORD">function</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">However</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="NAMED_ENTITY">BFGS</token>
      <token type="WORD">has</token>
      <token type="WORD">proven</token>
      <token type="WORD">to</token>
      <token type="WORD">have</token>
      <token type="WORD">good</token>
      <token type="WORD">performance</token>
      <token type="WORD">even</token>
      <token type="WORD">for</token>
      <token type="WORD">non-smooth</token>
      <token type="WORD">optimizations</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
  </document>
  <document name="nlp_data/d10.txt">
    <sentence>
      <token type="NAMED_ENTITY">James Clerk Maxwell</token>
      <token type="WORD">was</token>
      <token type="WORD">a</token>
      <token type="WORD">Scottish</token>
      <token type="WORD">mathematical</token>
      <token type="WORD">physicist</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">His</token>
      <token type="WORD">most</token>
      <token type="WORD">notable</token>
      <token type="WORD">achievement</token>
      <token type="WORD">was</token>
      <token type="WORD">to</token>
      <token type="WORD">formulate</token>
      <token type="WORD">the</token>
      <token type="WORD">classical</token>
      <token type="WORD">theory</token>
      <token type="WORD">of</token>
      <token type="WORD">electromagnetic</token>
      <token type="WORD">radiation</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">bringing</token>
      <token type="WORD">together</token>
      <token type="WORD">for</token>
      <token type="WORD">the</token>
      <token type="WORD">first</token>
      <token type="WORD">time</token>
      <token type="WORD">electricity</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">magnetism</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">and</token>
      <token type="WORD">light</token>
      <token type="WORD">as</token>
      <token type="WORD">manifestations</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="WORD">same</token>
      <token type="WORD">phenomenon</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Maxwell</token>
      <token type="PUNCTUATION"><![CDATA[']]></token>
      <token type="WORD">s</token>
      <token type="WORD">equations</token>
      <token type="WORD">for</token>
      <token type="WORD">electromagnetism</token>
      <token type="WORD">have</token>
      <token type="WORD">been</token>
      <token type="WORD">called</token>
      <token type="WORD">the</token>
      <token type="PUNCTUATION"><![CDATA["]]></token>
      <token type="WORD">second</token>
      <token type="WORD">great</token>
      <token type="WORD">unification</token>
      <token type="WORD">in</token>
      <token type="WORD">physics</token>
      <token type="PUNCTUATION"><![CDATA["]]></token>
      <token type="WORD">after</token>
      <token type="WORD">the</token>
      <token type="WORD">first</token>
      <token type="WORD">one</token>
      <token type="WORD">realised</token>
      <token type="WORD">by</token>
      <token type="NAMED_ENTITY">Isaac Newton</token>
      <token type="PUNCTUATION"><![CDATA[...]]></token>
      <token type="WORD">Maxwell</token>
      <token type="WORD">demonstrated</token>
      <token type="WORD">that</token>
      <token type="WORD">electric</token>
      <token type="WORD">and</token>
      <token type="WORD">magnetic</token>
      <token type="WORD">fields</token>
      <token type="WORD">travel</token>
      <token type="WORD">through</token>
      <token type="WORD">space</token>
      <token type="WORD">as</token>
      <token type="WORD">waves</token>
      <token type="WORD">moving</token>
      <token type="WORD">at</token>
      <token type="WORD">the</token>
      <token type="WORD">speed</token>
      <token type="WORD">of</token>
      <token type="WORD">light</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">Maxwell</token>
      <token type="WORD">proposed</token>
      <token type="WORD">that</token>
      <token type="WORD">light</token>
      <token type="WORD">is</token>
      <token type="WORD">an</token>
      <token type="WORD">undulation</token>
      <token type="WORD">in</token>
      <token type="WORD">the</token>
      <token type="WORD">same</token>
      <token type="WORD">medium</token>
      <token type="WORD">that</token>
      <token type="WORD">is</token>
      <token type="WORD">the</token>
      <token type="WORD">cause</token>
      <token type="WORD">of</token>
      <token type="WORD">electric</token>
      <token type="WORD">and</token>
      <token type="WORD">magnetic</token>
      <token type="WORD">phenomena</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">The</token>
      <token type="WORD">unification</token>
      <token type="WORD">of</token>
      <token type="WORD">light</token>
      <token type="WORD">and</token>
      <token type="WORD">electrical</token>
      <token type="WORD">phenomena</token>
      <token type="WORD">led</token>
      <token type="WORD">to</token>
      <token type="WORD">the</token>
      <token type="WORD">prediction</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="WORD">existence</token>
      <token type="WORD">of</token>
      <token type="WORD">radio</token>
      <token type="WORD">waves</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">His</token>
      <token type="WORD">discoveries</token>
      <token type="WORD">helped</token>
      <token type="WORD">usher</token>
      <token type="WORD">in</token>
      <token type="WORD">the</token>
      <token type="WORD">era</token>
      <token type="WORD">of</token>
      <token type="WORD">modern</token>
      <token type="WORD">physics</token>
      <token type="PUNCTUATION"><![CDATA[,]]></token>
      <token type="WORD">laying</token>
      <token type="WORD">the</token>
      <token type="WORD">foundation</token>
      <token type="WORD">for</token>
      <token type="WORD">such</token>
      <token type="WORD">fields</token>
      <token type="WORD">as</token>
      <token type="WORD">special</token>
      <token type="WORD">relativity</token>
      <token type="WORD">and</token>
      <token type="WORD">quantum</token>
      <token type="WORD">mechanics</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">His</token>
      <token type="WORD">contributions</token>
      <token type="WORD">to</token>
      <token type="WORD">the</token>
      <token type="WORD">science</token>
      <token type="WORD">are</token>
      <token type="WORD">considered</token>
      <token type="WORD">by</token>
      <token type="WORD">many</token>
      <token type="WORD">to</token>
      <token type="WORD">be</token>
      <token type="WORD">of</token>
      <token type="WORD">the</token>
      <token type="WORD">same</token>
      <token type="WORD">magnitude</token>
      <token type="WORD">as</token>
      <token type="WORD">those</token>
      <token type="WORD">of</token>
      <token type="NAMED_ENTITY">Isaac Newton</token>
      <token type="WORD">and</token>
      <token type="NAMED_ENTITY">Albert Einstein</token>
      <token type="SENTENCE_TERMINAL">.</token>
    </sentence>
    <sentence>
      <token type="WORD">QED</token>
    </sentence>
  </document>
</archive>
